<?xml version="1.0" encoding="UTF-8"?>
<!--
 * Scilab ( http://www.scilab.org/ ) - This file is part of Scilab
 * Copyright (C) 2000 - INRIA - Carlos Klimann
 * 
 * This file must be used under the terms of the CeCILL.
 * This source file is licensed as described in the file COPYING, which
 * you should have received as part of this distribution.  The terms
 * are also available at    
 * http://www.cecill.info/licences/Licence_CeCILL_V2.1-en.txt
 *
 -->
<refentry xmlns="http://docbook.org/ns/docbook" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:svg="http://www.w3.org/2000/svg" xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:db="http://docbook.org/ns/docbook" xmlns:scilab="http://www.scilab.org" xml:lang="en" xml:id="pca">
    <refnamediv>
        <refname>pca</refname>
        <refpurpose>Computes  principal components analysis with  standardized variables</refpurpose>
    </refnamediv>
    <refsynopsisdiv>
        <title>Calling Sequence</title>
        <synopsis>[lambda,facpr,comprinc] = pca(x)</synopsis>
    </refsynopsisdiv>
    <refsection>
        <title>Arguments</title>
        <variablelist>
            <varlistentry>
                <term>x</term>
                <listitem>
                    <para> is a nxp (n individuals, p variables) real
                        matrix.
                    </para>
                    <para>
                        <note>
                            Note that <literal>pca</literal> center and
                            normalize the columns of <literal>x</literal> to produce
                            principal components analysis with standardized
                            variables.
                        </note>
                    </para>
                </listitem>
            </varlistentry>
            <varlistentry>
                <term>lambda</term>
                <listitem>
                    <para>is  a p x 2  numerical  matrix.  In the  first
                        column we  find the eigenvalues of  V, where V
                        is the correlation p x p matrix and in the second
                        column are the ratios of the corresponding 
                        eigenvalue over the sum of eigenvalues.
                    </para>
                </listitem>
            </varlistentry>
            <varlistentry>
                <term>facpr</term>
                <listitem>
                    <para>are the  principal  factors: eigenvectors  of
                        V. Each column is an eigenvector element of the
                        dual of <literal>R^p</literal>.
                    </para>
                </listitem>
            </varlistentry>
            <varlistentry>
                <term>comprinc</term>
                <listitem>
                    <para>are the  principal components.  Each column
                        (c_i=Xu_i)  of   this  n x n  matrix   is  the
                        M-orthogonal projection of  individuals onto
                        principal  axis.  Each one of this  columns
                        is a linear combination  of the variables
                        x1,   ...,xp  with   maximum   variance  under
                        condition <literal>u'_i M^(-1) u_i=1</literal>
                    </para>
                </listitem>
            </varlistentry>
        </variablelist>
    </refsection>
    <refsection>
        <title>Description</title>
        <para>This function performs several computations known as
            "principal component analysis".
        </para>
        <para>The idea behind this method is to represent in an
            approximative manner a cluster of n individuals in a smaller
            dimensional subspace.  In order to do that, it projects the
            cluster onto a subspace.  The choice of the k-dimensional
            projection subspace is made in such a way that the distances in
            the projection have a minimal deformation: we are looking for a
            k-dimensional subspace such that the squares of the distances in
            the projection is as big as possible (in fact in a projection,
            distances can only stretch).  In other words, inertia of the
            projection onto the k dimensional subspace must be maximal.
        </para>
        <para>Warning, the graphical part of the old version of
            <literal>pca</literal> has been removed. It can now be performed
            using the <link linkend="show_pca">show_pca</link>
            function.
        </para>
    </refsection>
    <refsection>
        <title>Examples</title>
        <programlisting role="example"><![CDATA[ 
a=rand(100,10,'n');
[lambda,facpr,comprinc] = pca(a);
show_pca(lambda,facpr)
 ]]></programlisting>
    </refsection>
    <refsection role="see also">
        <title>See Also</title>
        <simplelist type="inline">
            <member>
                <link linkend="show_pca">show_pca</link>
            </member>
            <member>
                <link linkend="princomp">princomp</link>
            </member>
        </simplelist>
    </refsection>
    <refsection>
        <title>Bibliography</title>
        <para>Saporta, Gilbert, Probabilites,  Analyse des
            Donnees et Statistique, Editions Technip, Paris, 1990.
        </para>
    </refsection>
</refentry>
